{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a1260c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from typing import Dict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48bcd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pat = pickle.load(open('all_pat.pkl', 'rb'))\n",
    "all_visits = pickle.load(open('all_visits.pkl', 'rb'))\n",
    "all_diags = pickle.load(open('all_diags.pkl', 'rb'))\n",
    "mortality = pickle.load(open('mortality.pkl', 'rb'))\n",
    "icd_dic = pickle.load(open('icd_dic.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "162ee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, seqs, hfs):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Store `seqs`. to `self.x` and `hfs` to `self.y`.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        Do NOT permute the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        self.x = seqs\n",
    "        self.y = hfs\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Return the number of samples (i.e. patients).\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        return len(self.y)\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Generates one sample of data.\n",
    "        \n",
    "        Note that you DO NOT need to covert them to tensor as we will do this later.\n",
    "        \"\"\"\n",
    "        \n",
    "        # your code here\n",
    "        seq = self.x[index]\n",
    "        hf = self.y[index]\n",
    "\n",
    "        return seq, hf\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "\n",
    "dataset = CustomDataset(all_diags, mortality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "788ecf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "\n",
    "\n",
    "    sequences, labels = zip(*data)\n",
    "    \n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "#     print(y)\n",
    "    num_patients = len(sequences)\n",
    "    num_visits = [len(patient) for patient in sequences]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "#     print(\"MAX\", max_num_visits, max_num_codes)\n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "#         print(\"AT First\", patient)\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            # your code here\n",
    "            \n",
    "            for k_diag, diag in enumerate(visit):\n",
    "                if diag != diag:\n",
    "                    continue\n",
    "                else:   \n",
    "                    x[i_patient, j_visit,k_diag ] = icd_dic[diag]\n",
    "                    \n",
    "                    masks[i_patient, j_visit,k_diag ] = 1\n",
    "        \n",
    "        \n",
    "        for j_visit, visit in enumerate(reversed(patient)):\n",
    "#             print(visit)\n",
    "            for k_diag, diag in enumerate(visit):\n",
    "                if diag != diag:\n",
    "                    continue\n",
    "                else: \n",
    "                    rev_x[i_patient, j_visit,k_diag ] = icd_dic[diag]\n",
    "                    rev_masks[i_patient, j_visit,k_diag ] = 1\n",
    "        for i in range(num_visits[i_patient], max_num_visits):\n",
    "            rev_masks[i_patient, i, :] = torch.zeros(max_num_codes)\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "707a0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train dataset: 32563\n",
      "Length of val dataset: 9769\n",
      "Length of test dataset: 4188\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "split = int(len(dataset)*0.7)\n",
    "\n",
    "lengths = [split, len(dataset) - split]\n",
    "train_dataset, val_test_dataset = random_split(dataset, lengths)\n",
    "\n",
    "\n",
    "split = int(len(val_test_dataset)*0.7)\n",
    "lengths = [split, len(val_test_dataset) - split]\n",
    "val_dataset, test_dataset = random_split(val_test_dataset, lengths)\n",
    "\n",
    "\n",
    "print(\"Length of train dataset:\", len(train_dataset))\n",
    "print(\"Length of val dataset:\", len(val_dataset))\n",
    "print(\"Length of test dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd9e97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data.dataset import random_split\n",
    "\n",
    "# split = int(len(dataset)*0.8)\n",
    "\n",
    "# lengths = [split, len(dataset) - split]\n",
    "# train_dataset, val_dataset = random_split(dataset, lengths)\n",
    "\n",
    "# print(\"Length of train dataset:\", len(train_dataset))\n",
    "# print(\"Length of val dataset:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21108075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def load_data(train_dataset, val_dataset,test_dataset, collate_fn):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "    Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "    Arguments:\n",
    "        train dataset: train dataset of type `CustomDataset`\n",
    "        val dataset: validation dataset of type `CustomDataset`\n",
    "        collate_fn: collate function\n",
    "        \n",
    "    Outputs:\n",
    "        train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "    Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "    '''\n",
    "    \n",
    "    batch_size = 128\n",
    "    # your code here\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "#     raise NotImplementedError\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = load_data(train_dataset, val_dataset, test_dataset, collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e4bf4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_embeddings_with_mask(x, masks):\n",
    "    \"\"\"\n",
    "    TODO: mask select the embeddings for true visits (not padding visits) and then\n",
    "        sum the embeddings for each visit up.\n",
    "\n",
    "    Arguments:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "        \n",
    "    NOTE: Do NOT use for loop.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    masks = torch.unsqueeze(masks, 3)\n",
    "    masks[:,:,:,0] = 1\n",
    "    masks = masks.expand(-1,-1,-1, x.shape[3])\n",
    "#     print(x.shape, masks.shape)\n",
    "    return torch.sum (x * masks, 2)\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abc38ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    \n",
    "    # your code here\n",
    "    b, v, e = hidden_states.shape\n",
    "    count = torch.count_nonzero(masks, 2)\n",
    "    pad = torch.zeros(b, v+1)\n",
    "    pad[:, :pad.shape[1]-1] = count + pad[:, :pad.shape[1]-1]\n",
    "    num_visits = torch.argmin(pad,1)\n",
    "    return hidden_states[range(b), num_visits-1,:]\n",
    "#     raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de2562b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaiveRNN(\n",
       "  (embedding): Embedding(6985, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "\n",
    "        # your code here\n",
    "        self.embedding = nn.Embedding(embedding_dim = 128, num_embeddings=num_codes)\n",
    "        self.rnn = nn.GRU(128, hidden_size = 128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(128, hidden_size=128, batch_first = True)\n",
    "        self.fc = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "#         raise NotImplementedError\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        true_h_n_rev = None\n",
    "        # your code here\n",
    "        rev_x = self.embedding(rev_x)\n",
    "        rev_x = sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        output, _ = self.rnn(rev_x)\n",
    "        true_h_n_rev = get_last_visit(output, rev_masks)\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(icd_dic))\n",
    "naive_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1f0b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d576f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "        \n",
    "    # your code here\n",
    "    p, r, f,_ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "#     raise NotImplementedError\n",
    "    return p, r, f, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "233109c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            loss = None\n",
    "            # your code here\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "#             raise NotImplementedError\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))\n",
    "        experiment.log_metric(\"precision\", p)\n",
    "        experiment.log_metric(\"ROC\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "739dc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/sayan3/general/c825e7c9251e49f9b5a75ae5ae2c1a8b\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     filename            : 1\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     notebook            : 1\n",
      "COMET INFO:     source_code         : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Couldn't find a Git repository in '/Users/AIUDD75/Library/CloudStorage/OneDrive-AmwayCorp/MCS/DL598/Final Project' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "COMET WARNING: Unknown error exporting current conda environment\n",
      "COMET WARNING: Unknown error retrieving Conda package as an explicit file\n",
      "COMET WARNING: Unknown error retrieving Conda information\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/sayan3/general/81632d2605cd4c6db74a234ecd91843f\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \t Training Loss: 0.358707\n",
      "Epoch: 1 \t Validation p: 0.58, r:0.01, f: 0.02, roc_auc: 0.81\n",
      "Epoch: 2 \t Training Loss: 0.284397\n",
      "Epoch: 2 \t Validation p: 0.64, r:0.26, f: 0.37, roc_auc: 0.88\n",
      "Epoch: 3 \t Training Loss: 0.241885\n",
      "Epoch: 3 \t Validation p: 0.62, r:0.51, f: 0.56, roc_auc: 0.90\n",
      "Epoch: 4 \t Training Loss: 0.219077\n",
      "Epoch: 4 \t Validation p: 0.61, r:0.52, f: 0.56, roc_auc: 0.90\n",
      "Epoch: 5 \t Training Loss: 0.204792\n",
      "Epoch: 5 \t Validation p: 0.62, r:0.55, f: 0.58, roc_auc: 0.91\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "\n",
    "# Initialize Comet.ml with your API key\n",
    "experiment = Experiment(api_key=\"dGXMo2OT0jVdMZPp2KVui30tD\", project_name=\"general\")\n",
    "\n",
    "n_epochs = 5\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa8cdff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8959743268134557\n"
     ]
    }
   ],
   "source": [
    "p, r, f, roc_auc = eval_model(naive_rnn, test_loader)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e746f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
